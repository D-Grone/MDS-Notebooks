{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONKTp2H0LTvz"
   },
   "source": [
    "# **Tarea 1**\n",
    "## Machine Learning Avanzado\n",
    "Integrantes: Patricio Ramirez\n",
    "             Carlos Bustamante\n",
    "             Nicolas Rivera\n",
    "             Pablo Elgueta\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bxvw-PXNYdu"
   },
   "source": [
    "##### Objetivo:\n",
    "En este laboratorio volveremos a ver la biblioteca del trimestre anterior de perros y gatos. la base de datos de imagenes se\n",
    "encuentra en la siguiente dirección: ‘https://www.kaggle.com/competitions/dogs-vs-cats/data‘\n",
    "Ademas, para completar el análisis, se le pide construir una red que permita hacer la clasificación en base a los audios\n",
    "adicionalmente: ‘https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs‘ \n",
    "\n",
    "###### Se le evaluará por:\n",
    "\n",
    "- Carga y lectura de imagenes\n",
    "- Crear una red neuronal convolucional\n",
    "- Crear una red neuronal LSTM\n",
    "- Pruebe 2 formas distintas de arquitectura de red para cada una.\n",
    "- Cargue dos fotos de su biblioteca (sí, vaya y saque una foto a un perro o gato).\n",
    "- Evalúe sus modelos con las fotos sacadas.\n",
    "- Cargue los datos de audio\n",
    "- Proponga un modelo para clasificar este tipo de dato\n",
    "- Evalúe su modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFcTdPzdQqvi"
   },
   "source": [
    "### Dogs vs Cats\n",
    "\n",
    "#### **Descripción Dataset**\n",
    "\n",
    "\n",
    "- El archivo de entrenamiento contiene 25.000 imagenes de perros y gatos. \n",
    "- Entrena tu algoritmo con estos archivos y predice las etiquetas para test1.zip \n",
    "\n",
    "#### **Etiquetas:**\n",
    "\n",
    "- 1 = dog \n",
    "- 0 = cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGcnCC0UTcq9"
   },
   "source": [
    "## 1.-Reconocimiento e importación de librerías\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1hHs4kkfdnD"
   },
   "source": [
    "Para manipular bases de datos se usó Pandas, para visualización fueron las librerías Matplotlib y Seaborn, y en cuanto a Machine Learning se utilizó Keras y Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVzlvDlYaICU",
    "outputId": "8d998c8b-670f-4a84-b844-03b761213a30"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "#Analisis de Datos\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Visualizacion de Datos\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Machine Learning\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "\n",
    "#test = pd.read_csv(\"/home/pato/Magister Data Science/Tercer Trimestre Data Science/ML Avanzado/Tareas/titanic/test.csv\")\n",
    "#train = pd.read_csv(\"/home/pato/Magister Data Science/Tercer Trimestre Data Science/ML Avanzado/Tareas/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevas\n",
    "\n",
    "from matplotlib.image import imread\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.-Cargando el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'od' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22092\\2477755621.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.kaggle.com/c/outbrain-click-prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"username\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"dgrone\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"key\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"8e16739c70ce24ea1e6d9a45b61e9796\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'od' is not defined"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/competitions/dogs-vs-cats/data\")\n",
    "{\"username\":\"dgrone\",\"key\":\"8e16739c70ce24ea1e6d9a45b61e9796\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(file, folder, password):\n",
    "    import zipfile\n",
    "    ruta_zip = file\n",
    "    ruta_extraccion = folder\n",
    "    archivo_zip = zipfile.ZipFile(ruta_zip, 'r')\n",
    "    try:\n",
    "        archivo_zip.extractall(pwd=password, path=ruta_extraccion)\n",
    "    except:\n",
    "        pass\n",
    "    archivo_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "folder = 'dogs-vs-cats/train/'\n",
    "if 'train' not in  listdir('dogs-vs-cats/'):\n",
    "    file = 'dogs-vs-cats/train.zip'\n",
    "    password = None\n",
    "    folder = 'dogs-vs-cats'\n",
    "    extract_zip(file, folder, password)\n",
    "    folder = 'dogs-vs-cats/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from os import listdir\n",
    "folder = 'dogs-vs-cats/train/'\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "for i in range(len(listdir(folder))):\n",
    "    if listdir(folder)[i].startswith('cat'):\n",
    "        j = j+1\n",
    "        print(j)\n",
    "    if listdir(folder)[i].startswith('dog'):\n",
    "        k = k+1\n",
    "    \n",
    "print(f'cats images = {j}')\n",
    "print(f'cdogs images = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    pyplot.subplot(130  + 1 + i)\n",
    "    filename = folder + 'dog.' + str(i) + '.jpg'\n",
    "    image = imread(filename)\n",
    "    pyplot.imshow(image)\n",
    "pyplot.show()\n",
    "\n",
    "for i in range(3):\n",
    "    pyplot.subplot(130  + 1 + i)\n",
    "    filename = folder + 'cat.' + str(i) + '.jpg'\n",
    "    image = imread(filename)\n",
    "    pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generaremos un subset de Perros para poder probar con menos tiempo de procesamiento\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "i=0\n",
    "photos = list()\n",
    "labels = list()\n",
    "for image in range(12500):   # en versión final se debe reemplazar por file in listdir(folder), hay que probar\n",
    "    file = 'dog.'+ str(i) + '.jpg'\n",
    "    photo = load_img(folder + file, target_size=(50, 50))\n",
    "    photo = img_to_array(photo)\n",
    "    photos.append(photo)\n",
    "    if file.startswith('dog'):\n",
    "        label = 1.0\n",
    "        labels.append(label)\n",
    "    file = 'cat.'+ str(i) + '.jpg'\n",
    "    photo = load_img(folder + file, target_size=(50, 50))\n",
    "    photo = img_to_array(photo)\n",
    "    photos.append(photo)\n",
    "    if file.startswith('cat'):\n",
    "        label = 0.0\n",
    "        labels.append(label)\n",
    "photos = asarray(photos)\n",
    "labels = asarray(labels)\n",
    "print(photos.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "save('dogs_vs_cats_photos.npy', photos)\n",
    "save('dogs_vs_cats_labels.npy', labels)\n",
    "photos = load('dogs_vs_cats_photos.npy')\n",
    "labels = load('dogs_vs_cats_labels.npy')\n",
    "print(photos.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = photos/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = photos[1].shape\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(photos, labels,test_size=0.3,random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow\n",
    "\n",
    "## Primero, el modelo es secuencial\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "## agregamos capa convolucional 2D\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=(3,3),\n",
    "        activation='relu',\n",
    "        input_shape = input_shape\n",
    "    )\n",
    ")\n",
    "\n",
    "## agregamos segunda capa convolucional 2D\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Conv2D(\n",
    "        64,\n",
    "        kernel_size=(3,3),\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "## agregamos capa de Pooling\n",
    "model.add(\n",
    "    tensorflow.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2,2)\n",
    "    )\n",
    ")\n",
    "\n",
    "## agregamos capa de regularizacion Dropout para minimizar calculos\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Dropout(\n",
    "        0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "## agregamos capa de aplanado\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Flatten())\n",
    "\n",
    "## agregamos capa de conexion completa\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Dense(\n",
    "        units=128,\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "## agregamos capa de regularizacion Dropout para minimizar calculos\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Dropout(\n",
    "        0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "## agregamos capa de conexion completa\n",
    "model.add(\n",
    "    tensorflow.keras.layers.Dense(\n",
    "        units=num_classes,\n",
    "        activation='softmax'\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs=20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose = 1,validation_data=(X_test,y_test))\n",
    "\n",
    "score = model.evaluate(X_test,y_test,verbose = 0)\n",
    "print('Test loss: ',score[0])\n",
    "print('Test accuract: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo más Preciso\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), activation = 'relu', input_shape = input_shape))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "# Add another:\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación Mejor Accuracy\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 1, validation_data=(X_test,y_test) )\n",
    "score = model.evaluate(X_test,y_test,verbose = 0)\n",
    "print('Test loss: ',score[0])\n",
    "print('Test accuract: ',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# load dogs vs cats dataset, reshape and save to a new file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from numpy import save\n",
    "\n",
    "photos, labels = list(), list()\n",
    "# enumerate files in the directory\n",
    "for file in listdir(folder):\n",
    "\t# determine class\n",
    "\toutput = 0.0\n",
    "\n",
    "\tlabels.append(output)\n",
    "# convert to a numpy arrays\n",
    "\n",
    "\n",
    "print(photos.shape, labels.shape)\n",
    "# save the reshaped photos\n",
    "save('dogs_vs_cats_photos.npy', photos)\n",
    "save('dogs_vs_cats_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = keras.models.Sequential()\n",
    "\n",
    "modeloCNN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeloCNN.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeloCNN.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 1, validation_data=(X_test,y_test) )\n",
    "score = model.evaluate(X_test,y_test,verbose = 0)\n",
    "print('Test loss: ',score[0])\n",
    "print('Test accuract: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN2 = keras.models.Sequential()\n",
    "\n",
    "modeloCNN2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(250, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación Mejor Accuracy\n",
    "\n",
    "modeloCNN2.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modeloCNN2.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 1, validation_data=(X_test,y_test) )\n",
    "score = model.evaluate(X_test,y_test,verbose = 0)\n",
    "print('Test loss: ',score[0])\n",
    "print('Test accuract: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "tensorboardCNN2 = TensorBoard(log_dir='logs/cnn2')\n",
    "modeloCNN2.fit(X_train, y_train, batch_size=32,\n",
    "                validation_split=0.15,\n",
    "                epochs=12,\n",
    "                callbacks=[tensorboardCNN2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN3 = keras.models.Sequential()\n",
    "\n",
    "modeloCNN3 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(250, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación Mejor Accuracy\n",
    "\n",
    "modeloCNN3.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function and optimizer\n",
    "modeloCNN3.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# train the parameters\n",
    "history = modeloCNN3.fit(X_train, y_train, epochs=epochs, batch_size=16)\n",
    "\n",
    "# evaluate accuracy\n",
    "train_acc = modeloCNN3.evaluate(X_train, y_train, batch_size=16)[1]\n",
    "test_acc = modeloCNN3.evaluate(X_test, y_test, batch_size=16)[1]\n",
    "print('Training accuracy: %s' % train_acc)\n",
    "print('Testing accuracy: %s' % test_acc)\n",
    "\n",
    "losses = history.history['loss']\n",
    "\n",
    "plt.plot(range(len(losses)), losses, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X4lnewdWfn6"
   },
   "source": [
    "## 1.2.-Procesamiento de datos y análisis descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5CXO_ZrpdAH"
   },
   "source": [
    "a) Se observa una distribución de edades similar entre hombres y mujeres, sin embargo, los hombres poseen los registros con las edades mas grandes.\n",
    "\n",
    "b) La cantidad de hombres a bordo casi duplica a la de mujeres.\n",
    "\n",
    "c) 342 personas sobrevivieron, correspondientes al 38%.\n",
    "\n",
    "d) El 74% de mujeres sobrevivió, mientras que en los hombres solo un 19% lo hizo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "PehtVH4bxOHs",
    "outputId": "3d5176bc-2e3e-48a4-decd-5af4d485e348"
   },
   "outputs": [],
   "source": [
    "train[['Survived','PassengerId','Sex']].groupby(['Sex','Survived']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "X4eNfY0lf4e8",
    "outputId": "bae5248d-43dd-4acb-c5c7-f25953e996b3"
   },
   "outputs": [],
   "source": [
    "#Representación de Heatmap todas las variables contínuas\n",
    "ax.set_title(\"Heatmap considerando sólo variables continuas\")\n",
    "corrMatrix_cont = train[['Age','SibSp', 'Parch', 'Fare']].corr('pearson')\n",
    "sns.heatmap(corrMatrix_cont, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xPW7XbxyAxf"
   },
   "source": [
    "No se aprecian correlaciones fuertes, siendo la mas significativa la de hermanos/parejas con padres e hijos, sugiriendo que algunos pasajeros viajaban en familia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAq5yj_Oz0cJ",
    "outputId": "266b485c-5009-403f-80b5-154a6c3a0a79"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6doETI_jNH6"
   },
   "source": [
    "Al comprobar los valores nulos usando la función info, es posible detectar que la variable **Age** tiene 177 registros vacíos, **Cabin** tiene 687 y **Embarked** tiene 2.\n",
    "\n",
    "En **Age**, los nulos serán rellenados con la edad promedio de la muestra, en **Cabin**, los valores nulos representan a las personas que no iban en una pieza particular, y en **Embarked** fueron rellenados con la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B92rGDM5hQnv",
    "outputId": "ac65738b-5dbd-41a8-a732-0b5a2e6d866a"
   },
   "outputs": [],
   "source": [
    "#Quitamos columnas con informacion unica\n",
    "train.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True, )\n",
    "\n",
    "#Binarizar columna Cabin\n",
    "train[\"Cabin\"] = train[\"Cabin\"].astype(str) != 'nan'\n",
    "train[\"Cabin\"] = train[\"Cabin\"].astype(int)\n",
    "\n",
    "#Rellenando los NA de la edad con la media\n",
    "median_age = train['Age'].median()\n",
    "train['Age'].fillna(median_age, inplace=True)\n",
    "train['Age']=train['Age'].astype(int)\n",
    "\n",
    "#Se rellenan 2 na de Embarked con la moda (S)\n",
    "embarked_mode = train['Embarked'].mode()\n",
    "train['Embarked'].fillna(embarked_mode.values[0], inplace=True)\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "3DJ044MiQXBJ",
    "outputId": "58dfd4df-5158-4efb-f22f-0e53efd1f98a"
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='Age', data=train, hue='Survived')\n",
    "#Se crea una variable categorica para la edad con el proposito de separar a las personas menores a 14 anos\n",
    "train['Age_cat'] = pd.cut(train['Age'], bins=[-1, 14, float('Inf')], labels=['kid', 'adult'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHX8MtLC1bTT"
   },
   "source": [
    "El análisis de la distribución de la edad en los supervivientes permite observar que las personas menores de 14 años son mas propensas a sobrevivir, es por eso que se genera una variable categórica para agruparlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "Jadqhvlh_fGY",
    "outputId": "7e721342-b7bd-43ac-9bdc-0e5e8c267223"
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='Fare', data=train, hue='Survived')\n",
    "#Se puede crear también una variable categórica para las tarifas mayores a 50.\n",
    "train['Fare_cat'] = pd.cut(train['Fare'], bins=[-1, 35, float('Inf')], labels=['cheap', 'expensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceXKi-9H2TKo"
   },
   "source": [
    "Ocurre una situación similar con las tarifas, por eso se agruparon entre menores y mayores a $35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "YomVzMdMacIm",
    "outputId": "a1c091d5-8fec-4d38-f8fe-e6c93ffbb848"
   },
   "outputs": [],
   "source": [
    "train[['Pclass', 'Survived']].groupby(['Pclass']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "D2rYw-cZaxIk",
    "outputId": "2e452867-fa5d-4ae1-c42d-c4bc0dabdd6b"
   },
   "outputs": [],
   "source": [
    "train[['Embarked', 'Pclass', 'Survived']].groupby(['Embarked', 'Pclass']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wVXTn2kQBy54",
    "outputId": "e659b714-25a1-450b-bee5-28fad15a36e2"
   },
   "outputs": [],
   "source": [
    "#Binarización de las variables categóricas\n",
    "train[\"Sex\"] = train['Sex'].replace(['female','male'],[0,1])\n",
    "train[\"Sex\"] = train[\"Sex\"].astype(int)\n",
    "Age_dum = pd.get_dummies(train['Age_cat'], prefix = \"age\")\n",
    "Fare_dum = pd.get_dummies(train['Fare_cat'], prefix = \"fare\")\n",
    "Embarked_dum = pd.get_dummies(train['Embarked'], prefix = \"embarked\")\n",
    "Pclass_dum = pd.get_dummies(train['Pclass'], prefix = \"pclass\")\n",
    "train.head()\n",
    "#Añadimos las nuevas variables, ahora como dummies, al dataset\n",
    "train = pd.concat([train, Embarked_dum, Age_dum, Fare_dum, Pclass_dum], axis = 1)\n",
    "#Se borran también algunos dummies para asegurar invertibilidad de la matríz \n",
    "train = train.drop(columns = ['Pclass','Age', 'Fare', 'Embarked',\"Age_cat\",\"age_adult\", \"Fare_cat\",\"embarked_C\",\"fare_expensive\",'pclass_1'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e-vsDEhL4dLj",
    "outputId": "99fc7ebd-80c8-40ab-a18e-a05ce27501da"
   },
   "outputs": [],
   "source": [
    "#Aplicamos las mismas operaciones sobre la data de testeo\n",
    "#Tratamiento NaNs y columnas nuevas\n",
    "test.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True, )\n",
    "test[\"Cabin\"] = test[\"Cabin\"].astype(str) != 'nan'\n",
    "test[\"Cabin\"] = test[\"Cabin\"].astype(int)\n",
    "median_age2 = test['Age'].median()\n",
    "test['Age'].fillna(median_age2, inplace=True)\n",
    "test['Age']=test['Age'].astype(int)\n",
    "embarked_mode2 = test['Embarked'].mode()\n",
    "test['Embarked'].fillna(embarked_mode2.values[0], inplace=True)\n",
    "test['Age_cat'] = pd.cut(test['Age'], bins=[-1, 14, float('Inf')], labels=['kid', 'adult'])\n",
    "test['Fare_cat'] = pd.cut(test['Fare'], bins=[-1, 35, float('Inf')], labels=['cheap', 'expensive'])\n",
    "\n",
    "#Binarización de las variables categóricas\n",
    "test[\"Sex\"] = test['Sex'].replace(['female','male'],[0,1])\n",
    "test[\"Sex\"] = test[\"Sex\"].astype(int)\n",
    "Age_dum2 = pd.get_dummies(test['Age_cat'], prefix = \"age\")\n",
    "Fare_dum2 = pd.get_dummies(test['Fare_cat'], prefix = \"fare\")\n",
    "Embarked_dum2 = pd.get_dummies(test['Embarked'], prefix = \"embarked\")\n",
    "Pclass_dum = pd.get_dummies(test['Pclass'], prefix = \"pclass\")\n",
    "\n",
    "#Añadimos las nuevas variables, ahora como dummies, al dataset\n",
    "test = pd.concat([test, Embarked_dum2, Age_dum2, Fare_dum2, Pclass_dum], axis = 1)\n",
    "#Se borran también algunos dummies para asegurar invertibilidad de la matríz \n",
    "test = test.drop(columns = ['Pclass','Age', 'Fare', 'Embarked',\"Age_cat\",\"age_adult\", \"Fare_cat\",\"embarked_C\",\"fare_expensive\",\"pclass_1\"])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['Sex', 'age_kid','SibSp','Parch','embarked_S','embarked_Q','fare_cheap','pclass_2','pclass_3','Cabin','Survived']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['Sex', 'age_kid','SibSp','Parch','embarked_S','embarked_Q','fare_cheap','pclass_2','pclass_3','Cabin']]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Entrenamiento de Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Datasets de Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuDLgtzN0XAH"
   },
   "outputs": [],
   "source": [
    "train_data = train.iloc[:,0:len(train.columns)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe de Variable Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PldY2Ry0W9R"
   },
   "outputs": [],
   "source": [
    "train_target = train.iloc[:,len(train.columns)-1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lista de Características de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns.values\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separación de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjHqSC2q0W6c"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_target,test_size=0.3,random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalización de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkURcC6h0W38"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler \n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "scaler.fit(test)\n",
    "\n",
    "test = scaler.transform(test)\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJADBlSe0W1x"
   },
   "source": [
    "### 2.2.-Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Buk4s1p00Wyv"
   },
   "source": [
    "##### Selección de Modelo Secuencial de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8J7s8hv0WwI"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "best_test_accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1H87eGT0Wtb"
   },
   "source": [
    "##### Modelo 1  de Red Neuronal\n",
    "Se escoje un momedelo inicial de 3 capas con las siguientes características:\n",
    "\n",
    "- learning rate = 0.01\n",
    "- Capa 1 = 'relu', 20 nodos\n",
    "- Capa 2 = 'relu', 20 nodos\n",
    "- Capa 3 = 'sigmoid', 1 nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsxsfIGb0WrC"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Capa 1\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu', input_dim=len(features)))\n",
    "# Capa 2\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa de Salida\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ60Y9FF0WoZ"
   },
   "outputs": [],
   "source": [
    "# define our loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "# train the parameters\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
    "\n",
    "# evaluate accuracy\n",
    "train_acc = model.evaluate(X_train, y_train, batch_size=16)[1]\n",
    "test_acc = model.evaluate(X_test, y_test, batch_size=16)[1]\n",
    "print('Training accuracy: %s' % train_acc)\n",
    "print('Testing accuracy: %s' % test_acc)\n",
    "\n",
    "losses = history.history['loss']\n",
    "\n",
    "plt.plot(range(len(losses)), losses, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_test_accuracy < test_acc:\n",
    "    best_test_accuracy = test_acc\n",
    "    model_best = model\n",
    "\n",
    "print(f'MODELO:\\n-------------------------------------------------------------------')\n",
    "model.summary()\n",
    "i=0\n",
    "for layer in model.layers:\n",
    "    i=i+1\n",
    "    print(f'Función Activación Capa {i} = {layer.activation}')\n",
    "print(f'\\nPrecisión:{test_acc}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l4Q3fCY0WYo"
   },
   "source": [
    "##### Modelo 2 de Red Neuronal\n",
    "- learning rate = 0.008\n",
    "- Capa 1 = 'relu', 20 nodos\n",
    "- Capa 2 = 'relu', 20 nodos\n",
    "- Capa 3 = 'relu', 10 nodos\n",
    "- Capa 4 = 'relu', 20 nodos\n",
    "- Capa 5 = 'relu', 10 nodos\n",
    "- Capa 6 = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Capa 1\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu', input_dim=len(features)))\n",
    "# Capa 2\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa 3\n",
    "model.add(Dense(10, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa 4\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa 5\n",
    "model.add(Dense(10, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa de Salida\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.008), metrics=['accuracy'])\n",
    "\n",
    "# train the parameters\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
    "\n",
    "# evaluate accuracy\n",
    "train_acc = model.evaluate(X_train, y_train, batch_size=16)[1]\n",
    "test_acc = model.evaluate(X_test, y_test, batch_size=16)[1]\n",
    "print('Training accuracy: %s' % train_acc)\n",
    "print('Testing accuracy: %s' % test_acc)\n",
    "\n",
    "losses = history.history['loss']\n",
    "\n",
    "plt.plot(range(len(losses)), losses, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_test_accuracy < test_acc:\n",
    "    best_test_accuracy = test_acc\n",
    "    model_best = model\n",
    "\n",
    "print(f'MODELO:\\n-------------------------------------------------------------------')\n",
    "model.summary()\n",
    "i=0\n",
    "for layer in model.layers:\n",
    "    i=i+1\n",
    "    print(f'Función Activación Capa {i} = {layer.activation}')\n",
    "print(f'\\nPrecisión:{test_acc}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 3  de Red Neuronal\n",
    "- learning rate = 0.01\n",
    "- Capa 1 = 'relu'\n",
    "- Capa 2 = 'sigmoid'\n",
    "- Capa 3 = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Capa 1\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu', input_dim=len(features)))\n",
    "# Capa 2\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='sigmoid'))\n",
    "# Capa de Salida\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "# train the parameters\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
    "\n",
    "# evaluate accuracy\n",
    "train_acc = model.evaluate(X_train, y_train, batch_size=16)[1]\n",
    "test_acc = model.evaluate(X_test, y_test, batch_size=16)[1]\n",
    "print('Training accuracy: %s' % train_acc)\n",
    "print('Testing accuracy: %s' % test_acc)\n",
    "\n",
    "losses = history.history['loss']\n",
    "\n",
    "plt.plot(range(len(losses)), losses, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_test_accuracy < test_acc:\n",
    "    best_test_accuracy = test_acc\n",
    "    model_best = model\n",
    "\n",
    "print(f'MODELO:\\n-------------------------------------------------------------------')\n",
    "model.summary()\n",
    "i=0\n",
    "for layer in model.layers:\n",
    "    i=i+1\n",
    "    print(f'Función Activación Capa {i} = {layer.activation}')\n",
    "print(f'\\nPrecisión:{test_acc}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 4 de Red Neuronal\n",
    "- learning rate = 0.009\n",
    "- Capa 1 = 'relu', 20 nodos\n",
    "- Capa 2 = 'relu', 20 nodos\n",
    "- Capa 3 = 'relu', 10 nodos\n",
    "- Capa 4 = 'relu', 20 nodos\n",
    "- Capa 5 = 'relu', 10 nodos\n",
    "- Capa 6 = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Capa 1\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu', input_dim=len(features)))\n",
    "# Capa 2\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa 3\n",
    "model.add(Dense(10, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa 4\n",
    "model.add(Dense(20, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa 5\n",
    "model.add(Dense(10, kernel_initializer = 'uniform', activation='relu'))\n",
    "# Capa de Salida\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.009), metrics=['accuracy'])\n",
    "\n",
    "# train the parameters\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
    "\n",
    "# evaluate accuracy\n",
    "train_acc = model.evaluate(X_train, y_train, batch_size=16)[1]\n",
    "test_acc = model.evaluate(X_test, y_test, batch_size=16)[1]\n",
    "print('Training accuracy: %s' % train_acc)\n",
    "print('Testing accuracy: %s' % test_acc)\n",
    "\n",
    "losses = history.history['loss']\n",
    "\n",
    "plt.plot(range(len(losses)), losses, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_test_accuracy < test_acc:\n",
    "    best_test_accuracy = test_acc\n",
    "    model_best = model\n",
    "\n",
    "print(f'MODELO:\\n-------------------------------------------------------------------')\n",
    "model.summary()\n",
    "i=0\n",
    "for layer in model.layers:\n",
    "    i=i+1\n",
    "    print(f'Función Activación Capa {i} = {layer.activation}')\n",
    "print(f'\\nPrecisión:{test_acc}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Pruebas de Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a las pruebas realizadas con variados modelos con diferentes características y ajustes de parámetros, hemos optado por considerar cuatro modelos de diferentes caracteristicas( learning rate, cantidad de capas, función de activación y cantidad de nodos).\n",
    "\n",
    "En base a lo anterior, los criterios utilizados para realizar el ajuste y selección de los modelos fue de acuerdo a los siguientes criterios:\n",
    " \n",
    " ##### 1.- Al aplicar una función de activación a “sigmoid” estamos proyectando una salida que sea de 0 a 1 y eso nos ayudaría de mejor manera a clasificar ya sea si sobrevive (1) o no sobrevive(0).\n",
    "##### 2.- El optimizador Adam combina las bondades de Adagrad y RMSprop este algoritmo que es mas reciente que los anteriores y esta construido en base a sus predecesores se esperaria que su rendimiento sea superior.\n",
    "##### 3.- Para las funciones de coste se utilizara binary cross entropy para el clasificador binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MEJOR MODELO:\\n-------------------------------------------------------------------')\n",
    "model_best.summary()\n",
    "i=0\n",
    "for layer in model_best.layers:\n",
    "    i=i+1\n",
    "    print(f'Función Activación Capa {i} = {layer.activation}')\n",
    "print(f'\\nPrecisión:{best_test_accuracy}' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=((model_best.predict(test) > 0.5).astype('int32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_survived = pd.DataFrame(predict_x, columns=['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test, columns= ['Sex', 'age_kid','SibSp','Parch','embarked_S','embarked_Q','fare_cheap','pclass_2','pclass_3','Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_complete = pd.concat([test_df, test_survived], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_complete\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se preparan las visualizaciones \n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (12,8))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "axes = axes.flatten()\n",
    "ax = axes[0]\n",
    "# Distribución de los datos según sexo\n",
    "sns.countplot(x='Sex', data=data, palette=['#73DAF5','#F5C5F5'],ax=ax)\n",
    "ax.set_xticklabels(['Hombre','Mujer'])\n",
    "ax.set_title('b) Distribución por Sexo')\n",
    "ax = axes[2]\n",
    "# Revisamos que tan balanceadas estan nuestras clases\n",
    "sns.countplot(x='Survived', data=data, palette=['#F57373','#8FF573'],ax=ax)\n",
    "ax.set_title('c) Distribución de variable Target')\n",
    "ax.set_xticklabels(['No sobrevive','Sobrevive'],rotation=0)\n",
    "ax = axes[3]\n",
    "#Frecuencia de diagnóstico por sexo\n",
    "pd.crosstab(test_complete.Sex,test_complete.Survived).plot(kind=\"bar\",ax=ax,color=['#8FF573','#F57373'])\n",
    "ax.set_title('d) Frecuencia de diagnóstico por Sexo')\n",
    "ax.set_xticklabels(['Hombre','Mujer'],rotation=0)\n",
    "ax.legend([\"Sobrevive\", \"No sobrevive\"])\n",
    "ax.set_ylabel('Frecuencia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.- Resultado Predictivo Para Los Miembros del Equipo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ingresan los valores señalados por cada miembro del grupo, mas un 'dummy' para poder ejecutar de manera correcta la estandarizacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patricio = {'Sex':1, 'age_kid':0, 'SibSp':0, 'Parch':0, 'embarked_S':1, 'embarked_Q':0, 'fare_cheap':1, 'pclass_2':1, 'pclass_3':0, 'Cabin':1}\n",
    "Carlos = {'Sex':1, 'age_kid':0, 'SibSp':0, 'Parch':0, 'embarked_S':0, 'embarked_Q':0, 'fare_cheap':0, 'pclass_2':0, 'pclass_3':0, 'Cabin':1}\n",
    "Nicolas = {'Sex':1, 'age_kid':0, 'SibSp':0, 'Parch':0, 'embarked_S':0, 'embarked_Q':1, 'fare_cheap':0, 'pclass_2':1, 'pclass_3':0, 'Cabin':1}\n",
    "Pablo = {'Sex':1, 'age_kid':0, 'SibSp':0, 'Parch':0, 'embarked_S':1, 'embarked_Q':0, 'fare_cheap':0, 'pclass_2':0, 'pclass_3':0, 'Cabin':1}\n",
    "Dummy = {'Sex':0, 'age_kid':0, 'SibSp':0, 'Parch':0, 'embarked_S':1, 'embarked_Q':0, 'fare_cheap':0, 'pclass_2':0, 'pclass_3':0, 'Cabin':0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predict = pd.DataFrame(columns=['Sex', 'age_kid','SibSp','Parch','embarked_S','embarked_Q','fare_cheap','pclass_2','pclass_3','Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predict = Grupo_predict.append([Patricio, Carlos, Nicolas, Pablo, Dummy], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predict = Grupo_predict.astype(int)\n",
    "Grupo_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Grupo_predict)\n",
    "\n",
    "Grupo_predict = scaler.transform(Grupo_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grupo=((model_best.predict(Grupo_predict) > 0.5).astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predicted = pd.DataFrame(Grupo_predict, columns= ['Sex', 'age_kid','SibSp','Parch','embarked_S','embarked_Q','fare_cheap','pclass_2','pclass_3','Cabin'])\n",
    "\n",
    "Grupo_predicted['Nombres'] = ['Patricio', 'Carlos', 'Nicolas', 'Pablo', 'Dummy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupo_predicted = Grupo_predicted.drop(Grupo_predicted.index[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_survived = pd.DataFrame(predict_grupo, columns=['Survived'])\n",
    "Grupo_complete = pd.concat([Grupo_predicted, Grupo_survived], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo_complete = Grupo_complete.drop(Grupo_complete.index[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Grupo_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dado que tres de cuatro hombres sobreviven, nos damos cuenta de que el valor que tenemos como threshold para el predict de 0.5 hace que algunos de nosotros sobrevivan, siendo que lo mas probable es que todos estabamos un poco mas arriba que 0.5. Mientras que la persona que murio estaba un poco mas abajo del 0.5\n",
    "\n",
    "### Cuando se usa un umbral de 0.8, ya nadie sobrevive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En conclusion, se logra dar un contexto general de la data, hacer una limpieza y visualizacion la cual hace que la data se pueda entrenar para asi lograr el objetivo de la tarea la cual es implementar un modelo de red neuronal y ver quienes sobreviven y quienes no. Asi, logramos aprender e identificar los distintos parametros los cuales componen al modelo secuencial, como la funcion de activacion, el learning rate, el optimizador, las distintas capas, el numero de neuronas y la funcion de costo.\n",
    "\n",
    "### Por ultimo, logramos identificar los mejores modelos para despues utilizarlos en la prediccion de los datos de prueba y para los datos del grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
