{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288cb707",
   "metadata": {},
   "source": [
    "## Machine Learning Avanzado\n",
    "#### Redes Recursivas y Recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447a8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Parametros iniciales\n",
    "velocidadDeAprendizaje = 0.001\n",
    "numeroDeIteracionesEntrenamiento = 5000\n",
    "iteracionesMostrarInfo = 1000\n",
    "numeroEntradas = 3\n",
    "\n",
    "# Numero de unidades ocultas en una celda RNN\n",
    "hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c0613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
    "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
    "input_dim = 28\n",
    "\n",
    "units = 64\n",
    "output_size = 10  # labels are from 0 to 9\n",
    "\n",
    "# Build the RNN model\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    if allow_cudnn_kernel:\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
    "    else:\n",
    "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "        lstm_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
    "        )\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(output_size),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e95c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "sample, sample_label = x_train[0], y_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302c4e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 16s 14ms/step - loss: 0.9545 - accuracy: 0.7004 - val_loss: 0.5849 - val_accuracy: 0.8054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x183f6f35fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(allow_cudnn_kernel=True)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de77b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 15s 14ms/step - loss: 0.4023 - accuracy: 0.8810 - val_loss: 0.3706 - val_accuracy: 0.8829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x183f8a8ca90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
    "noncudnn_model.set_weights(model.get_weights())\n",
    "noncudnn_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "noncudnn_model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0e4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result is: [3], target result is: 5\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
    "    cpu_model.set_weights(model.get_weights())\n",
    "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
    "    print(\n",
    "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
    "    )\n",
    "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))\n",
    "\n",
    "print(sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "833aa202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_to_module = \"/home/tom/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/\"\n",
    "sys.path.append(path_to_module)\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08683d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/tom/Fonte Storage/00 Clases/UDD/MLA/Laboratorios/5.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m test_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file,cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    451\u001b[0m     warn_deprecated(\n\u001b[0;32m    452\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    455\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2640\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   2636\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2637\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2638\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[0;32m   2639\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2640\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   2641\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm, aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   2642\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation, alpha\u001b[38;5;241m=\u001b[39malpha, vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   2643\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax, origin\u001b[38;5;241m=\u001b[39morigin, extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   2644\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   2645\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm, filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   2646\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   2647\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2648\u001b[0m     sci(__ret)\n\u001b[0;32m   2649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    451\u001b[0m     warn_deprecated(\n\u001b[0;32m    452\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    455\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5488\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5482\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap, norm, interpolation,\n\u001b[0;32m   5483\u001b[0m                       origin, extent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   5484\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   5485\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5486\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 5488\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5489\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5491\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\image.py:706\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be converted to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;66;03m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = '/media/tom/Fonte Storage/00 Clases/UDD/MLA/Laboratorios/5.png'\n",
    "test_image = cv2.imread(file,cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(test_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized = cv2.resize(test_image,(28,28),interpolation = cv2.INTER_LINEAR)\n",
    "img_resized = cv2.bitwise_not(img_resized)\n",
    "plt.imshow(img_resized,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label = 3#\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
    "    cpu_model.set_weights(model.get_weights())\n",
    "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(img_resized, 0)), axis=1)\n",
    "    print(\n",
    "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
    "    )\n",
    "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8ddf3",
   "metadata": {},
   "source": [
    "### Redes Recursivas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero hay que representar el arbol de la estructura de datos en un grafico\n",
    "\n",
    "class Node:  # a node in the tree\n",
    "    def __init__(self, ...):\n",
    "        self.isLeaf = True / False\n",
    "        self.hidden_state = None\n",
    "        # for leaves\n",
    "        self.word = word\n",
    "        # for inner nodes\n",
    "        self.left = None  # reference to left child\n",
    "        self.right = None  # reference to right child\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model():\n",
    "    def add_model_vars(self):\n",
    "        with tf.variable_scope('Embeddings'):\n",
    "            embeddings = \\\n",
    "            tf.get_variable('embeddings', [len(self.vocab), self.config.embed_size])\n",
    "            with tf.variable_scope('Composition'):\n",
    "                W1 = tf.get_variable('W1',\n",
    "                                     [2 * self.config.embed_size, self.config.embed_size])\n",
    "                b1 = tf.get_variable('b1', [1, self.config.embed_size])\n",
    "            with tf.variable_scope('Projection'):\n",
    "                U = tf.get_variable('U', [self.config.embed_size, self.config.label_size])\n",
    "                bs = tf.get_variable('bs', [1, self.config.label_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys,shutil,time,itertools\n",
    "import math, random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "import tree\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    Model objects are passed a Config() object at instantiation.\n",
    "    \"\"\"\n",
    "    \n",
    "    embed_size = 35\n",
    "    label_size = 2\n",
    "    early_stopping = 2\n",
    "    anneal_threshold = 0.99\n",
    "    anneal_by = 1.5\n",
    "    max_epochs = 30\n",
    "    lr = 0.01\n",
    "    l2 = 0.02\n",
    "    \n",
    "    model_name = MODEL_STR % (embed_size, l2, lr)\n",
    "\n",
    "\n",
    "class RecursiveNetStaticGraph():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # Load train data and build vocabulary\n",
    "        self.train_data, self.dev_data, self.test_data = tree.simplified_data(700,\n",
    "                                                                              100,\n",
    "                                                                              200)\n",
    "        \n",
    "        # print(\"data \",self.train_data)\n",
    "        self.vocab = utils.Vocab()\n",
    "        train_sents = [t.get_words() for t in self.train_data]\n",
    "        self.vocab.construct(list(itertools.chain.from_iterable(train_sents)))\n",
    "        \n",
    "        # add input placeholders\n",
    "        self.is_leaf_placeholder = tf.placeholder(\n",
    "            tf.bool, (None), name='is_leaf_placeholder')\n",
    "        self.left_children_placeholder = tf.placeholder(\n",
    "            tf.int32, (None), name='left_children_placeholder')\n",
    "        self.right_children_placeholder = tf.placeholder(\n",
    "            tf.int32, (None), name='right_children_placeholder')\n",
    "        self.node_word_indices_placeholder = tf.placeholder(\n",
    "            tf.int32, (None), name='node_word_indices_placeholder')\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            tf.int32, (None), name='labels_placeholder')\n",
    "        \n",
    "        # add model variables\n",
    "        with tf.variable_scope('Embeddings'):\n",
    "            embeddings = tf.get_variable('embeddings',\n",
    "                                         [len(self.vocab), self.config.embed_size])\n",
    "        with tf.variable_scope('Composition'):\n",
    "            W1 = tf.get_variable('W1',\n",
    "                                 [2 * self.config.embed_size, self.config.embed_size])\n",
    "            b1 = tf.get_variable('b1', [1, self.config.embed_size])\n",
    "        with tf.variable_scope('Projection'):\n",
    "            U = tf.get_variable('U', [self.config.embed_size, self.config.label_size])\n",
    "            bs = tf.get_variable('bs', [1, self.config.label_size])\n",
    "            \n",
    "        # build recursive graph\n",
    "        \n",
    "        tensor_array = tf.TensorArray(\n",
    "            tf.float32,\n",
    "            size=0,\n",
    "            dynamic_size=True,\n",
    "            clear_after_read=False,\n",
    "            infer_shape=False)\n",
    "        \n",
    "        def embed_word(word_index):\n",
    "            with tf.device('/cpu:0'):\n",
    "                return tf.expand_dims(tf.gather(embeddings, word_index), 0)\n",
    "\n",
    "    def combine_children(left_tensor, right_tensor):\n",
    "        return tf.nn.relu(tf.matmul(tf.concat([left_tensor, right_tensor],1), W1) + b1)\n",
    "    \n",
    "    def loop_body(tensor_array, i):\n",
    "        node_is_leaf = tf.gather(self.is_leaf_placeholder, i)\n",
    "        node_word_index = tf.gather(self.node_word_indices_placeholder, i)\n",
    "        left_child = tf.gather(self.left_children_placeholder, i)\n",
    "        right_child = tf.gather(self.right_children_placeholder, i)\n",
    "        print(left_child,\"left_child\")\n",
    "        node_tensor = tf.cond(\n",
    "            node_is_leaf,\n",
    "            lambda: embed_word(node_word_index),\n",
    "            lambda: combine_children(tensor_array.read(left_child),\n",
    "                                     tensor_array.read(right_child)))\n",
    "        tensor_array = tensor_array.write(i, node_tensor)\n",
    "        i = tf.add(i, 1)\n",
    "        return tensor_array, i\n",
    "    \n",
    "    loop_cond = lambda tensor_array, i: \\\n",
    "        tf.less(i, tf.squeeze(tf.shape(self.is_leaf_placeholder)))\n",
    "    self.tensor_array, _ = tf.while_loop(loop_cond, loop_body, [tensor_array, 0], parallel_iterations=1)\n",
    "    \n",
    "    # add projection layer\n",
    "    self.logits = tf.matmul(self.tensor_array.concat(), U) + bs\n",
    "    self.root_logits = tf.matmul(\n",
    "        self.tensor_array.read(self.tensor_array.size() - 1), U) + bs\n",
    "    self.root_prediction = tf.squeeze(tf.argmax(self.root_logits, 1))\n",
    "    \n",
    "    # add loss layer\n",
    "    regularization_loss = self.config.l2 * (\n",
    "        tf.nn.l2_loss(W1) + tf.nn.l2_loss(U))\n",
    "    included_indices = tf.where(tf.less(self.labels_placeholder, 2))\n",
    "    self.full_loss = regularization_loss + tf.reduce_sum(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=tf.gather(self.logits, included_indices),labels=tf.gather(\n",
    "                self.labels_placeholder, included_indices)))\n",
    "    self.root_loss = tf.reduce_sum(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=self.root_logits,labels=self.labels_placeholder[-1:]))\n",
    "    \n",
    "    # add training op\n",
    "    self.train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(\n",
    "        self.full_loss)\n",
    "    \n",
    "    def build_feed_dict(self, node):\n",
    "        nodes_list = []\n",
    "        tree.leftTraverse(node, lambda node, args: args.append(node), nodes_list)\n",
    "        node_to_index = OrderedDict()\n",
    "        for i in xrange(len(nodes_list)):\n",
    "            node_to_index[nodes_list[i]] = i\n",
    "        feed_dict = {\n",
    "            self.is_leaf_placeholder: [node.isLeaf for node in nodes_list],\n",
    "            self.left_children_placeholder: [node_to_index[node.left] if\n",
    "                                             not node.isLeaf else -1\n",
    "                                             for node in nodes_list],\n",
    "            self.right_children_placeholder: [node_to_index[node.right] if\n",
    "                                              not node.isLeaf else -1\n",
    "                                              for node in nodes_list],\n",
    "            self.node_word_indices_placeholder: [self.vocab.encode(node.word) if\n",
    "                                                 node.word else -1\n",
    "                                                 for node in nodes_list],\n",
    "            self.labels_placeholder: [node.label for node in nodes_list]\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def predict(self, trees, weights_path, get_loss=False):\n",
    "        \"\"\"Make predictions from the provided model.\"\"\"\n",
    "        results = []\n",
    "        losses = []\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, weights_path)\n",
    "            for tree in trees:\n",
    "                feed_dict = self.build_feed_dict(tree.root)\n",
    "                if get_loss:\n",
    "                    root_prediction, loss = sess.run(\n",
    "                        [self.root_prediction, self.root_loss], feed_dict=feed_dict)\n",
    "                    losses.append(loss)\n",
    "                else:\n",
    "                    root_prediction = sess.run(self.root_prediction, feed_dict=feed_dict)\n",
    "                    results.append(root_prediction)\n",
    "            return results, losses\n",
    "\n",
    "    def run_epoch(self, new_model=False, verbose=True):\n",
    "        loss_history = []\n",
    "        # training\n",
    "        random.shuffle(self.train_data)\n",
    "        with tf.Session() as sess:\n",
    "            if new_model:\n",
    "                sess.run(tf.initialize_all_variables())\n",
    "            else:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess, SAVE_DIR + '%s.temp' % self.config.model_name)\n",
    "            for step, tree in enumerate(self.train_data):\n",
    "                print(tree,\"tree\")\n",
    "                feed_dict = self.build_feed_dict(tree.root)\n",
    "                loss_value, _ = sess.run([self.full_loss, self.train_op],\n",
    "                                         feed_dict=feed_dict)\n",
    "                loss_history.append(loss_value)\n",
    "                if verbose:\n",
    "                    sys.stdout.write('\\r{} / {} :    loss = {}'.format(step, len(\n",
    "                        self.train_data), np.mean(loss_history)))\n",
    "                    sys.stdout.flush()\n",
    "                    saver = tf.train.Saver()\n",
    "                    if not os.path.exists(SAVE_DIR):\n",
    "                        os.makedirs(SAVE_DIR)\n",
    "                    saver.save(sess, SAVE_DIR + '%s.temp' % self.config.model_name)\n",
    "            # statistics\n",
    "            train_preds, _ = self.predict(self.train_data,\n",
    "                                          SAVE_DIR + '%s.temp' % self.config.model_name)\n",
    "            val_preds, val_losses = self.predict(\n",
    "                self.dev_data,\n",
    "                SAVE_DIR + '%s.temp' % self.config.model_name,\n",
    "                get_loss=True)\n",
    "            train_labels = [t.root.label for t in self.train_data]\n",
    "            val_labels = [t.root.label for t in self.dev_data]\n",
    "            train_acc = np.equal(train_preds, train_labels).mean()\n",
    "            val_acc = np.equal(val_preds, val_labels).mean()\n",
    "            \n",
    "            print()\n",
    "            print( 'Training acc (only root node): {}'.format(train_acc))\n",
    "            print( 'Valiation acc (only root node): {}'.format(val_acc))\n",
    "            print( self.make_conf(train_labels, train_preds))\n",
    "            print( self.make_conf(val_labels, val_preds))\n",
    "            return train_acc, val_acc, loss_history, np.mean(val_losses)\n",
    "\n",
    "    def train(self, verbose=True):\n",
    "        complete_loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        prev_epoch_loss = float('inf')\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_epoch = 0\n",
    "        stopped = -1\n",
    "        for epoch in xrange(self.config.max_epochs):\n",
    "            print( 'epoch %d' % epoch)\n",
    "            if epoch == 0:\n",
    "                train_acc, val_acc, loss_history, val_loss = self.run_epoch(\n",
    "                    new_model=True)\n",
    "            else:\n",
    "                train_acc, val_acc, loss_history, val_loss = self.run_epoch()\n",
    "            complete_loss_history.extend(loss_history)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            #lr annealing\n",
    "            epoch_loss = np.mean(loss_history)\n",
    "            if epoch_loss > prev_epoch_loss * self.config.anneal_threshold:\n",
    "                self.config.lr /= self.config.anneal_by\n",
    "                print( 'annealed lr to %f' % self.config.lr)\n",
    "            prev_epoch_loss = epoch_loss\n",
    "\n",
    "        #save if model has improved on val\n",
    "        if val_loss < best_val_loss:\n",
    "            shutil.copyfile(SAVE_DIR + '%s.temp' % self.config.model_name,\n",
    "                            SAVE_DIR + '%s' % self.config.model_name)\n",
    "            best_val_loss = val_loss\n",
    "            best_val_epoch = epoch\n",
    "\n",
    "        # if model has not imprvoved for a while stop\n",
    "        if epoch - best_val_epoch > self.config.early_stopping:\n",
    "            stopped = epoch\n",
    "            #break\n",
    "        if verbose:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print( '\\n\\nstopped at %d\\n' % stopped)\n",
    "        return {\n",
    "            'loss_history': complete_loss_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def make_conf(self, labels, predictions):\n",
    "        confmat = np.zeros([2, 2])\n",
    "        for l, p in itertools.izip(labels, predictions):\n",
    "            confmat[l, p] += 1\n",
    "        return confmat\n",
    "\n",
    "def plot_loss_history(stats):\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('loss_history.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_RNN():\n",
    "    \"\"\"Test RNN model implementation.\n",
    "    \"\"\"\n",
    "    config = Config()\n",
    "    model = RecursiveNetStaticGraph(config)\n",
    "    #graph_def = tf.get_default_graph().as_graph_def()\n",
    "    #with open('static_graph.pb', 'wb') as f:\n",
    "    #  f.write(graph_def.SerializeToString())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    stats = model.train(verbose=True)\n",
    "    print( 'Training time: {}'.format(time.time() - start_time))\n",
    "    \n",
    "    plot_loss_history(stats)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    val_preds, val_losses = model.predict(\n",
    "        model.dev_data,\n",
    "        SAVE_DIR + '%s.temp' % model.config.model_name,\n",
    "        get_loss=True)\n",
    "    \n",
    "    val_labels = [t.root.label for t in model.dev_data]\n",
    "    val_acc = np.equal(val_preds, val_labels).mean()\n",
    "    print( val_acc)\n",
    "    \n",
    "    print( '-' * 20)\n",
    "    print( 'Test')\n",
    "    predictions, _ = model.predict(model.test_data,\n",
    "                                   SAVE_DIR + '%s.temp' % model.config.model_name)\n",
    "    labels = [t.root.label for t in model.test_data]\n",
    "    print(model.make_conf(labels, predictions))\n",
    "    test_acc = np.equal(predictions, labels).mean()\n",
    "    print('Test acc: {}'.format(test_acc))\n",
    "    print('Inference time, dev+test: {}'.format(time.time() - start_time))\n",
    "    print('-' * 20)\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        test_RNN()\n",
    "  # train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aec0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'the': 0, 'old': 1, 'cat': 2}\n",
    "node_words = ['the', 'old', 'cat', '', '']\n",
    "is_leaf = [True, True, True, False, False]\n",
    "left_children = [-1, -1, -1, 1, 0]   # indices of left children nodes in this list\n",
    "right_children = [-1, -1, -1, 2, 3]  # indices of right children nodes in this list\n",
    "\n",
    "node_word_indices = [vocab[word] if word else -1 for word in node_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_tensors = tf.TensorArray(tf.float32, size=0, dynamic_size=True,\n",
    "                              clear_after_read=False, infer_shape=False)\n",
    "\n",
    "def embed_word(word_index):\n",
    "    with tf.device('/cpu:0'):\n",
    "        return tf.expand_dims(tf.gather(embeddings, word_index), 0)\n",
    "\n",
    "def combine_children(left_tensor, right_tensor):\n",
    "    return tf.nn.relu(tf.matmul(tf.concat(1, [left_tensor, right_tensor]), W1) + b1)\n",
    "\n",
    "def loop_body(node_tensors, i):\n",
    "    node_is_leaf = tf.gather(is_leaf, i)\n",
    "    node_word_index = tf.gather(node_word_indices, i)\n",
    "    left_child = tf.gather(left_children, i)\n",
    "    right_child = tf.gather(right_children, i)\n",
    "    node_tensor = tf.cond(\n",
    "        node_is_leaf,\n",
    "        lambda: embed_word(node_word_index),\n",
    "        lambda: combine_children(node_tensors.read(left_child),\n",
    "                                 node_tensors.read(right_child)))\n",
    "    node_tensors = node_tensors.write(i, node_tensor)\n",
    "    i = tf.add(i, 1)\n",
    "    return node_tensors, i\n",
    "\n",
    "loop_cond = lambda node_tensors, i: \\\n",
    "        tf.less(i, tf.squeeze(tf.shape(is_leaf)))\n",
    "\n",
    "node_tensors, _ = tf.while_loop(loop_cond, loop_body, [node_tensors, 0],\n",
    "                                     parallel_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281add89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Embedding as embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5c031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
